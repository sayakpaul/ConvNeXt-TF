{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "zpiEiO2BUeO5",
   "metadata": {
    "id": "zpiEiO2BUeO5"
   },
   "source": [
    "# Off-the-shelf image classification with ConvNeXt models on TF-Hub\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/sayakpaul/ConvNeXt-TF/blob/main/notebooks/classification.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/sayakpaul/ConvNeXt-TF/blob/main/notebooks/classification.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://tfhub.dev/sayakpaul/collections/convnext/1\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\" />See TF Hub models</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661e6538",
   "metadata": {
    "id": "661e6538"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b73e50-6538-4af5-9878-ed99489409f5",
   "metadata": {
    "id": "f2b73e50-6538-4af5-9878-ed99489409f5"
   },
   "outputs": [],
   "source": [
    "!wget https://storage.googleapis.com/bit_models/ilsvrc2012_wordnet_lemmas.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43974820-4eeb-4b3a-90b4-9ddfa00d1cb9",
   "metadata": {
    "id": "43974820-4eeb-4b3a-90b4-9ddfa00d1cb9"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z5l1cRpiSavW",
   "metadata": {
    "id": "z5l1cRpiSavW"
   },
   "source": [
    "## Select a ConvNeXt ImageNet-1k model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0wM8idaSaOq",
   "metadata": {
    "cellView": "form",
    "id": "a0wM8idaSaOq"
   },
   "outputs": [],
   "source": [
    "model_name = \"convnext_tiny_1k_224\" #@param [\"convnext_tiny_1k_224\", \"convnext_small_1k_224\", \"convnext_base_1k_224\", \"convnext_base_1k_384\", \"convnext_large_1k_224\", \"convnext_large_1k_384\", \"convnext_base_21k_1k_224\", \"convnext_base_21k_1k_384\", \"convnext_large_21k_1k_224\", \"convnext_large_21k_1k_384\", \"convnext_xlarge_21k_1k_224\", \"convnext_xlarge_21k_1k_384\"]\n",
    "\n",
    "model_handle_map ={\n",
    "    \"convnext_tiny_1k_224\": \"https://tfhub.dev/sayakpaul/convnext_tiny_1k_224/1\",\n",
    "    \"convnext_small_1k_224\": \"https://tfhub.dev/sayakpaul/convnext_small_1k_224/1\",\n",
    "    \"convnext_base_1k_224\": \"https://tfhub.dev/sayakpaul/convnext_base_1k_224/1\",\n",
    "    \"convnext_base_1k_384\": \"https://tfhub.dev/sayakpaul/convnext_base_1k_384/1\",\n",
    "    \"convnext_large_1k_224\": \"https://tfhub.dev/sayakpaul/convnext_large_1k_224/1\",\n",
    "    \"convnext_large_1k_384\": \"https://tfhub.dev/sayakpaul/convnext_large_1k_384/1\",\n",
    "    \"convnext_base_21k_1k_224\": \"https://tfhub.dev/sayakpaul/convnext_base_21k_1k_224/1\",\n",
    "    \"convnext_base_21k_1k_384\": \"https://tfhub.dev/sayakpaul/convnext_base_21k_1k_384/1\",\n",
    "    \"convnext_large_21k_1k_224\": \"https://tfhub.dev/sayakpaul/convnext_large_21k_1k_224/1\",\n",
    "    \"convnext_large_21k_1k_384\": \"https://tfhub.dev/sayakpaul/convnext_large_21k_1k_384/1\",\n",
    "    \"convnext_xlarge_21k_1k_224\": \"https://tfhub.dev/sayakpaul/convnext_xlarge_21k_1k_224/1\",\n",
    "    \"convnext_xlarge_21k_1k_384\": \"https://tfhub.dev/sayakpaul/convnext_xlarge_21k_1k_384/1\",\n",
    "\n",
    "}\n",
    "\n",
    "input_resolution = int(model_name.split(\"_\")[-1])\n",
    "model_handle = model_handle_map[model_name]\n",
    "print(f\"Input resolution: {input_resolution} x {input_resolution} x 3.\")\n",
    "print(f\"TF-Hub handle: {model_handle}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441b5361",
   "metadata": {
    "id": "441b5361"
   },
   "source": [
    "## Image preprocessing utilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e76ff1-e1e0-4c6a-91b2-4114aad60e5b",
   "metadata": {
    "id": "63e76ff1-e1e0-4c6a-91b2-4114aad60e5b"
   },
   "outputs": [],
   "source": [
    "crop_layer = keras.layers.CenterCrop(224, 224)\n",
    "norm_layer = keras.layers.Normalization(\n",
    "    mean=[0.485 * 255, 0.456 * 255, 0.406 * 255],\n",
    "    variance=[(0.229 * 255) ** 2, (0.224 * 255) ** 2, (0.225 * 255) ** 2],\n",
    ")\n",
    "\n",
    "\n",
    "def preprocess_image(image, size=input_resolution):\n",
    "    image = np.array(image)\n",
    "    image_resized = tf.expand_dims(image, 0)\n",
    "    \n",
    "    if size == 224:\n",
    "        image_resized = tf.image.resize(image_resized, (256, 256), method=\"bicubic\")\n",
    "        image_resized = crop_layer(image_resized)\n",
    "    elif size == 384:\n",
    "        image_resized = tf.image.resize(image, (size, size), method=\"bicubic\")\n",
    "    \n",
    "    return norm_layer(image_resized).numpy()\n",
    "    \n",
    "\n",
    "def load_image_from_url(url):\n",
    "    # Credit: Willi Gierke\n",
    "    response = requests.get(url)\n",
    "    image = Image.open(BytesIO(response.content))\n",
    "    preprocessed_image = preprocess_image(image)\n",
    "    return image, preprocessed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b961e14",
   "metadata": {
    "id": "8b961e14"
   },
   "source": [
    "## Load ImageNet-1k labels and a demo image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc9250a-5eb6-4547-8893-dd4c746ab53b",
   "metadata": {
    "id": "8dc9250a-5eb6-4547-8893-dd4c746ab53b"
   },
   "outputs": [],
   "source": [
    "with open(\"ilsvrc2012_wordnet_lemmas.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "imagenet_int_to_str = [line.rstrip() for line in lines]\n",
    "\n",
    "img_url = \"https://p0.pikrepo.com/preview/853/907/close-up-photo-of-gray-elephant.jpg\"\n",
    "image, preprocessed_image = load_image_from_url(img_url)\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9006a643",
   "metadata": {
    "id": "9006a643"
   },
   "source": [
    "## Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfd2c7d-e454-48da-a40b-cd5d6f6c4908",
   "metadata": {
    "id": "8dfd2c7d-e454-48da-a40b-cd5d6f6c4908"
   },
   "outputs": [],
   "source": [
    "classification_model = tf.keras.Sequential(\n",
    "    [hub.KerasLayer(model_handle)]\n",
    ")  \n",
    "predictions = classification_model.predict(preprocessed_image)\n",
    "predicted_label = imagenet_int_to_str[int(np.argmax(predictions))]\n",
    "print(predicted_label)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "classification.ipynb",
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-7.m84",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m84"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
