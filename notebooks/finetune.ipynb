{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayakpaul/ConvNeXt-TF/blob/main/notebooks/finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89B27-TGiDNB"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9u3d4Z7uQsmp"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPo10cahZXXQ"
      },
      "source": [
        "## TPU/GPU detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpvUOuC3j27n"
      },
      "outputs": [],
      "source": [
        "try:  # detect TPUs\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()  # TPU detection\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "except ValueError:  # detect GPUs\n",
        "    tpu = False\n",
        "    strategy = (\n",
        "        tf.distribute.get_strategy()\n",
        "    )  # default strategy that works on CPU and single GPU\n",
        "print(\"Number of Accelerators: \", strategy.num_replicas_in_sync)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9S3uKC_iXY5"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCc6tdUGnD4C"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "IMAGE_SIZE = [224, 224]\n",
        "MODEL_PATH = \"https://tfhub.dev/sayakpaul/convnext_tiny_1k_224_fe/1\"\n",
        "\n",
        "# TPU\n",
        "if tpu:\n",
        "    BATCH_SIZE = (\n",
        "        16 * strategy.num_replicas_in_sync\n",
        "    )  # a TPU has 8 cores so this will be 128\n",
        "else:\n",
        "    BATCH_SIZE = 64  # on Colab/GPU, a higher batch size may throw(OOM)\n",
        "\n",
        "# Dataset\n",
        "CLASSES = [\n",
        "    \"dandelion\",\n",
        "    \"daisy\",\n",
        "    \"tulips\",\n",
        "    \"sunflowers\",\n",
        "    \"roses\",\n",
        "]  # don't change the order\n",
        "\n",
        "# Other constants\n",
        "MEAN = tf.constant([0.485 * 255, 0.456 * 255, 0.406 * 255])  # imagenet mean\n",
        "STD = tf.constant([0.229 * 255, 0.224 * 255, 0.225 * 255])  # imagenet std\n",
        "AUTO = tf.data.AUTOTUNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iTImGI5qMQT"
      },
      "source": [
        "# Data Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h29TLx7gqN_7"
      },
      "outputs": [],
      "source": [
        "def make_dataset(dataset: tf.data.Dataset, train: bool, image_size: int = IMAGE_SIZE):\n",
        "    def preprocess(image, label):\n",
        "        # for training, do augmentation\n",
        "        if train:\n",
        "            if tf.random.uniform(shape=[]) > 0.5:\n",
        "                image = tf.image.flip_left_right(image)\n",
        "        image = tf.image.resize(image, size=image_size, method=\"bicubic\")\n",
        "        image = (image - MEAN) / STD  # normalization\n",
        "        return image, label\n",
        "\n",
        "    if train:\n",
        "        dataset = dataset.shuffle(BATCH_SIZE * 10)\n",
        "\n",
        "    return dataset.map(preprocess, AUTO).batch(BATCH_SIZE).prefetch(AUTO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMQ3Qs9_pddU"
      },
      "source": [
        "# Flower Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3G-2aUBQJ-H"
      },
      "outputs": [],
      "source": [
        "train_dataset, val_dataset = tfds.load(\n",
        "    \"tf_flowers\",\n",
        "    split=[\"train[:90%]\", \"train[90%:]\"],\n",
        "    as_supervised=True,\n",
        "    try_gcs=False,  # gcs_path is necessary for tpu,\n",
        ")\n",
        "\n",
        "num_train = tf.data.experimental.cardinality(train_dataset)\n",
        "num_val = tf.data.experimental.cardinality(val_dataset)\n",
        "print(f\"Number of training examples: {num_train}\")\n",
        "print(f\"Number of validation examples: {num_val}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2X7sE3oRLXN"
      },
      "source": [
        "## Prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oftrfYw1qXei"
      },
      "outputs": [],
      "source": [
        "train_dataset = make_dataset(train_dataset, True)\n",
        "val_dataset = make_dataset(val_dataset, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNyCCM6PRM8I"
      },
      "source": [
        "## Visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaGzFUUVqjaC"
      },
      "outputs": [],
      "source": [
        "sample_images, sample_labels = next(iter(train_dataset))\n",
        "\n",
        "plt.figure(figsize=(5 * 3, 3 * 3))\n",
        "for n in range(15):\n",
        "    ax = plt.subplot(3, 5, n + 1)\n",
        "    image = (sample_images[n] * STD + MEAN).numpy()\n",
        "    image = (image - image.min()) / (\n",
        "        image.max() - image.min()\n",
        "    )  # convert to [0, 1] for avoiding matplotlib warning\n",
        "    plt.imshow(image)\n",
        "    plt.title(CLASSES[sample_labels[n]])\n",
        "    plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qf6u_7tt8BYy"
      },
      "source": [
        "# LR Scheduler Utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVTbnkJL79T_"
      },
      "outputs": [],
      "source": [
        "# Reference:\n",
        "# https://www.kaggle.com/ashusma/training-rfcx-tensorflow-tpu-effnet-b2\n",
        "\n",
        "\n",
        "class WarmUpCosine(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(\n",
        "        self, learning_rate_base, total_steps, warmup_learning_rate, warmup_steps\n",
        "    ):\n",
        "        super(WarmUpCosine, self).__init__()\n",
        "\n",
        "        self.learning_rate_base = learning_rate_base\n",
        "        self.total_steps = total_steps\n",
        "        self.warmup_learning_rate = warmup_learning_rate\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.pi = tf.constant(np.pi)\n",
        "\n",
        "    def __call__(self, step):\n",
        "        if self.total_steps < self.warmup_steps:\n",
        "            raise ValueError(\"Total_steps must be larger or equal to warmup_steps.\")\n",
        "        learning_rate = (\n",
        "            0.5\n",
        "            * self.learning_rate_base\n",
        "            * (\n",
        "                1\n",
        "                + tf.cos(\n",
        "                    self.pi\n",
        "                    * (tf.cast(step, tf.float32) - self.warmup_steps)\n",
        "                    / float(self.total_steps - self.warmup_steps)\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "\n",
        "        if self.warmup_steps > 0:\n",
        "            if self.learning_rate_base < self.warmup_learning_rate:\n",
        "                raise ValueError(\n",
        "                    \"Learning_rate_base must be larger or equal to \"\n",
        "                    \"warmup_learning_rate.\"\n",
        "                )\n",
        "            slope = (\n",
        "                self.learning_rate_base - self.warmup_learning_rate\n",
        "            ) / self.warmup_steps\n",
        "            warmup_rate = slope * tf.cast(step, tf.float32) + self.warmup_learning_rate\n",
        "            learning_rate = tf.where(\n",
        "                step < self.warmup_steps, warmup_rate, learning_rate\n",
        "            )\n",
        "        return tf.where(\n",
        "            step > self.total_steps, 0.0, learning_rate, name=\"learning_rate\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALtRUlxhw8Vt"
      },
      "source": [
        "# Model Utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JD9SI_Q9JdAB"
      },
      "outputs": [],
      "source": [
        "def get_model(model_path=MODEL_PATH, res=224, num_classes=5):\n",
        "    hub_layer = hub.KerasLayer(model_path, trainable=True)\n",
        "\n",
        "    model = keras.Sequential(\n",
        "        [\n",
        "            keras.layers.InputLayer((res, res, 3)),\n",
        "            hub_layer,\n",
        "            keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
        "        ]\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpZApp9u9_Y-"
      },
      "outputs": [],
      "source": [
        "get_model().summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMfenMQcxAAb"
      },
      "source": [
        "# Training Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1D7Iu7oD8WzX"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10\n",
        "WARMUP_STEPS = 10\n",
        "INIT_LR = 0.03\n",
        "WAMRUP_LR = 0.006\n",
        "\n",
        "TOTAL_STEPS = int((num_train / BATCH_SIZE) * EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmolHMov8als"
      },
      "outputs": [],
      "source": [
        "scheduled_lrs = WarmUpCosine(\n",
        "    learning_rate_base=INIT_LR,\n",
        "    total_steps=TOTAL_STEPS,\n",
        "    warmup_learning_rate=WAMRUP_LR,\n",
        "    warmup_steps=WARMUP_STEPS,\n",
        ")\n",
        "\n",
        "lrs = [scheduled_lrs(step) for step in range(TOTAL_STEPS)]\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(lrs)\n",
        "plt.xlabel(\"Step\", fontsize=14)\n",
        "plt.ylabel(\"LR\", fontsize=14)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-ID7vP5mIKs"
      },
      "outputs": [],
      "source": [
        "optimizer = keras.optimizers.SGD(scheduled_lrs)\n",
        "loss = keras.losses.SparseCategoricalCrossentropy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9p4ymNh9y7d"
      },
      "source": [
        "# Training & Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnZTSd8K90Mq"
      },
      "outputs": [],
      "source": [
        "with strategy.scope(): # this line is all that is needed to run on TPU (or multi-GPU, ...)\n",
        "  model = get_model(MODEL_PATH)\n",
        "  model.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(train_dataset, validation_data=val_dataset, epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jc7LMVz5Cbx6"
      },
      "outputs": [],
      "source": [
        "result = pd.DataFrame(history.history)\n",
        "fig, ax = plt.subplots(2, 1, figsize=(10, 10))\n",
        "result[[\"accuracy\", \"val_accuracy\"]].plot(xlabel=\"epoch\", ylabel=\"score\", ax=ax[0])\n",
        "result[[\"loss\", \"val_loss\"]].plot(xlabel=\"epoch\", ylabel=\"score\", ax=ax[1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKFMWzh0Yxsq"
      },
      "source": [
        "# Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMEsR851VDZb"
      },
      "outputs": [],
      "source": [
        "sample_images, sample_labels = next(iter(val_dataset))\n",
        "\n",
        "predictions = model.predict(sample_images, batch_size=16).argmax(axis=-1)\n",
        "evaluations = model.evaluate(sample_images, sample_labels, batch_size=16)\n",
        "\n",
        "print(\"[val_loss, val_acc]\", evaluations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzCCDL1CZFx6"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5 * 3, 3 * 3))\n",
        "for n in range(15):\n",
        "    ax = plt.subplot(3, 5, n + 1)\n",
        "    image = (sample_images[n] * STD + MEAN).numpy()\n",
        "    image = (image - image.min()) / (\n",
        "        image.max() - image.min()\n",
        "    )  # convert to [0, 1] for avoiding matplotlib warning\n",
        "    plt.imshow(image)\n",
        "    target = CLASSES[sample_labels[n]]\n",
        "    pred = CLASSES[predictions[n]]\n",
        "    plt.title(\"{} ({})\".format(target, pred))\n",
        "    plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e5oy9zmNNID"
      },
      "source": [
        "# Reference\n",
        "* [ConvNeXt-TF](https://github.com/sayakpaul/ConvNeXt-TF)\n",
        "* [Keras Flowers on TPU (solution)](https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/07_Keras_Flowers_TPU_solution.ipynb)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ConvNext-TF: Flower Classification (TPU/GPU).ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "environment": {
      "name": "tf22-cpu.2-2.m47",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf22-cpu.2-2:m47"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}