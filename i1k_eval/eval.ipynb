{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "926bae46",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bae636ae-24d1-4523-9997-696731318a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "from imutils import paths\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c4f0a2",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8238055-08bf-44e1-8f3b-98e7768f1603",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO = tf.data.AUTOTUNE\n",
    "BATCH_SIZE = 256\n",
    "IMAGE_SIZE = 224\n",
    "TF_MODEL_ROOT = \"gs://convnext/saved_models\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74edcf20",
   "metadata": {},
   "source": [
    "## Set up ImageNet-1k labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "334993ee-0d91-4572-9721-03e67af28cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"imagenet_class_index.json\", \"r\") as read_file:\n",
    "    imagenet_labels = json.load(read_file)\n",
    "\n",
    "MAPPING_DICT = {}\n",
    "LABEL_NAMES = {}\n",
    "for label_id in list(imagenet_labels.keys()):\n",
    "    MAPPING_DICT[imagenet_labels[label_id][0]] = int(label_id)\n",
    "    LABEL_NAMES[int(label_id)] = imagenet_labels[label_id][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01ad5447-3e28-4c86-941f-f64b45be603a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['val/n01751748/ILSVRC2012_val_00031060.JPEG',\n",
       "  'val/n01751748/ILSVRC2012_val_00013492.JPEG',\n",
       "  'val/n01751748/ILSVRC2012_val_00033108.JPEG',\n",
       "  'val/n01751748/ILSVRC2012_val_00021437.JPEG',\n",
       "  'val/n01751748/ILSVRC2012_val_00025096.JPEG'],\n",
       " [65, 65, 65, 65, 65])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_val_paths = list(paths.list_images(\"val\"))\n",
    "all_val_labels = [MAPPING_DICT[x.split(\"/\")[1]] for x in all_val_paths]\n",
    "\n",
    "all_val_paths[:5], all_val_labels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1124817d",
   "metadata": {},
   "source": [
    "## Preprocessing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a4f03d8-25d1-4660-9858-1b197425d5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare(path, label):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_png(image, channels=3)\n",
    "    image = tf.image.resize(image, (256, 256), method=\"bicubic\")\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0a2f457-ad4d-4cbd-8dfa-db544c7f6531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://github.com/facebookresearch/ConvNeXt/blob/main/datasets.py\n",
    "def get_preprocessing_model(input_size=224):\n",
    "    preprocessing_model = keras.Sequential()\n",
    "\n",
    "    preprocessing_model.add(layers.CenterCrop(input_size, input_size))\n",
    "    preprocessing_model.add(layers.Normalization(\n",
    "        mean=[0.485 * 255, 0.456 * 255, 0.406 * 255],\n",
    "        variance=[(0.229 * 255) ** 2, (0.224 * 255) ** 2, (0.225 * 255) ** 2],\n",
    "    ))\n",
    "\n",
    "    return preprocessing_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d33240",
   "metadata": {},
   "source": [
    "## Prepare `tf.data.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3518397-2ab0-4d79-adea-ae5f1cb66add",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-31 03:20:05.306146: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-31 03:20:05.828828: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38444 MB memory:  -> device: 0, name: A100-SXM4-40GB, pci bus id: 0000:00:04.0, compute capability: 8.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(None,), dtype=tf.int32, name=None))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = get_preprocessing_model()\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((all_val_paths, all_val_labels))\n",
    "dataset = dataset.map(load_and_prepare, num_parallel_calls=AUTO).batch(BATCH_SIZE)\n",
    "dataset = dataset.map(lambda x, y: (preprocessor(x), y), num_parallel_calls=AUTO)\n",
    "dataset = dataset.prefetch(AUTO)\n",
    "dataset.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea42076a",
   "metadata": {},
   "source": [
    "## Fetch model paths and filter the 224x224 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13a7e46e-31b2-48b9-9a57-2873fe27397a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['convnext_base_1k_224/', 'convnext_base_21k_1k_224/', 'convnext_large_1k_224/', 'convnext_large_21k_1k_224/', 'convnext_small_1k_224/', 'convnext_tiny_1k_224/', 'convnext_xlarge_21k_1k_224/']\n"
     ]
    }
   ],
   "source": [
    "model_paths = tf.io.gfile.listdir(TF_MODEL_ROOT)\n",
    "models_res_224 = [model_path for model_path in model_paths if str(IMAGE_SIZE) in model_path]\n",
    "p = re.compile('.*_21k_224')\n",
    "i1k_paths = [path for path in models_res_224 if not p.match(path)]\n",
    "\n",
    "print(i1k_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7e8e68",
   "metadata": {},
   "source": [
    "## Run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63a3da22-a60f-48b8-a0b0-02e54b2d012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_url):\n",
    "    classification_model = tf.keras.Sequential(\n",
    "        [\n",
    "            layers.InputLayer((224, 224, 3)),\n",
    "            hub.KerasLayer(model_url),\n",
    "        ]\n",
    "    )\n",
    "    return classification_model\n",
    "\n",
    "\n",
    "def evaluate_model(model_name):\n",
    "    tb_callback = tf.keras.callbacks.TensorBoard(log_dir=f\"logs_{model_name}\")\n",
    "    model_url = TF_MODEL_ROOT + \"/\" + model_name\n",
    "    \n",
    "    model = get_model(model_url)\n",
    "    model.compile(metrics=[\"accuracy\"])\n",
    "    _, accuracy = model.evaluate(dataset, callbacks=[tb_callback])\n",
    "    accuracy = round(accuracy * 100, 4)\n",
    "    print(f\"{model_name}: {accuracy}%.\", file=open(f\"{model_name.strip('/')}.txt\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4db846db-86bc-4ae8-a699-acb69331d93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating convnext_base_1k_224/.\n",
      "196/196 [==============================] - 118s 586ms/step - loss: 0.0000e+00 - accuracy: 0.8328\n",
      "Evaluating convnext_base_21k_1k_224/.\n",
      "196/196 [==============================] - 118s 585ms/step - loss: 0.0000e+00 - accuracy: 0.8536\n",
      "Evaluating convnext_large_1k_224/.\n",
      "196/196 [==============================] - 177s 879ms/step - loss: 0.0000e+00 - accuracy: 0.8384\n",
      "Evaluating convnext_large_21k_1k_224/.\n",
      "196/196 [==============================] - 175s 876ms/step - loss: 0.0000e+00 - accuracy: 0.8636\n",
      "Evaluating convnext_small_1k_224/.\n",
      "196/196 [==============================] - 92s 451ms/step - loss: 0.0000e+00 - accuracy: 0.8239\n",
      "Evaluating convnext_tiny_1k_224/.\n",
      "196/196 [==============================] - 60s 293ms/step - loss: 0.0000e+00 - accuracy: 0.8131\n",
      "Evaluating convnext_xlarge_21k_1k_224/.\n",
      "196/196 [==============================] - 241s 1s/step - loss: 0.0000e+00 - accuracy: 0.8673\n"
     ]
    }
   ],
   "source": [
    "for i1k_path in i1k_paths:\n",
    "    print(f\"Evaluating {i1k_path}.\")\n",
    "    evaluate_model(i1k_path)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-7.m84",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m84"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
